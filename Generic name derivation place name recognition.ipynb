{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-06T11:49:21.712728Z",
     "iopub.status.busy": "2024-06-06T11:49:21.712406Z",
     "iopub.status.idle": "2024-06-06T11:49:21.944361Z",
     "shell.execute_reply": "2024-06-06T11:49:21.942908Z",
     "shell.execute_reply.started": "2024-06-06T11:49:21.712703Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geo_derived_feature1.0.json\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-06T11:49:21.946099Z",
     "iopub.status.busy": "2024-06-06T11:49:21.945762Z",
     "iopub.status.idle": "2024-06-06T11:49:22.171514Z",
     "shell.execute_reply": "2024-06-06T11:49:22.170435Z",
     "shell.execute_reply.started": "2024-06-06T11:49:21.946069Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-06T11:49:22.173199Z",
     "iopub.status.busy": "2024-06-06T11:49:22.172816Z",
     "iopub.status.idle": "2024-06-06T11:49:25.580800Z",
     "shell.execute_reply": "2024-06-06T11:49:25.579593Z",
     "shell.execute_reply.started": "2024-06-06T11:49:22.173167Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/, https://mirrors.aliyun.com/pypi/simple/\r\n",
      "Collecting beautifulsoup4\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/b1/fe/e8c672695b37eecc5cbf43e1d0638d88d66ba3a44c4d321c796f4e59167f/beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m533.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting soupsieve>1.2 (from beautifulsoup4)\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/4c/f3/038b302fdfbe3be7da016777069f26ceefe11a681055ea1f7817546508e3/soupsieve-2.5-py3-none-any.whl (36 kB)\r\n",
      "Installing collected packages: soupsieve, beautifulsoup4\r\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.5\r\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-06T11:49:25.582723Z",
     "iopub.status.busy": "2024-06-06T11:49:25.582269Z",
     "iopub.status.idle": "2024-06-06T11:49:25.587430Z",
     "shell.execute_reply": "2024-06-06T11:49:25.586819Z",
     "shell.execute_reply.started": "2024-06-06T11:49:25.582681Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T11:49:25.588945Z",
     "iopub.status.busy": "2024-06-06T11:49:25.588368Z",
     "iopub.status.idle": "2024-06-06T11:49:30.538302Z",
     "shell.execute_reply": "2024-06-06T11:49:30.537314Z",
     "shell.execute_reply.started": "2024-06-06T11:49:25.588917Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\r\n",
      "  from .autonotebook import tqdm as notebook_tqdm\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import paddle\n",
    "import paddlehub as hub\n",
    "from paddle.io import Dataset\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#定义数据集\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self):\t\n",
    "        # 数据集存放位置\n",
    "        self.filepath = \"/home/aistudio/data/data273973/derived_place_name.json\"  #dataset_dir为数据集实际路径，需要填写全路径\n",
    "        self.data=pd.read_json(self.filepath)\n",
    "        self.data.reset_index(drop=True, inplace=True)\n",
    "    def __getitem__(self, idx):\n",
    "        toponym=self.data.loc[idx,'dname']\n",
    "        dfclass=self.data.loc[idx,'dfclass']\n",
    "        nfclass=self.data.loc[idx,'nfclass']\n",
    "        label=self.data.loc[idx,'label']\n",
    "        return {'input':{'toponym':toponym,'dfclass':dfclass,'nfclass':nfclass},'label':label}\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T11:49:30.540883Z",
     "iopub.status.busy": "2024-06-06T11:49:30.539590Z",
     "iopub.status.idle": "2024-06-06T11:49:30.809415Z",
     "shell.execute_reply": "2024-06-06T11:49:30.807577Z",
     "shell.execute_reply.started": "2024-06-06T11:49:30.540851Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File /home/aistudio/data/data273973/derived_place_name.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#构造训练数据集\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m inputDataset\u001b[38;5;241m=\u001b[39m\u001b[43mmyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#分割训练数据集、验证集\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trainDataset,valDataset\u001b[38;5;241m=\u001b[39mtrain_test_split(inputDataset,train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m ,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m34\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mmyDataset.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\t\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# 数据集存放位置\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/aistudio/data/data273973/derived_place_name.json\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m#dataset_dir为数据集实际路径，需要填写全路径\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/pandas/io/json/_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/pandas/io/json/_json.py:904\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/pandas/io/json/_json.py:960\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    952\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    959\u001b[0m ):\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    968\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File /home/aistudio/data/data273973/derived_place_name.json does not exist"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#构造训练数据集\n",
    "inputDataset=myDataset()\n",
    "#分割训练数据集、验证集\n",
    "trainDataset,valDataset=train_test_split(inputDataset,train_size=0.7,shuffle=True ,random_state=34)\n",
    "print(len(trainDataset),len(valDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-06T11:49:30.810310Z",
     "iopub.status.idle": "2024-06-06T11:49:30.810625Z",
     "shell.execute_reply": "2024-06-06T11:49:30.810483Z",
     "shell.execute_reply.started": "2024-06-06T11:49:30.810470Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import BertModel, BertTokenizer,BertForSequenceClassification\n",
    "from paddlenlp.transformers import ErnieForSequenceClassification,ErnieTokenizer\n",
    "import paddle.nn as nn\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model =BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model=ErnieForSequenceClassification.from_pretrained('ernie-2.0-base-en')\n",
    "tokenizer=ErnieTokenizer.from_pretrained('ernie-2.0-base-en')\n",
    "# import paddle.nn as nn\n",
    "# # \n",
    "# class Model(nn.Layer):\n",
    "#     def __init__(self, model):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.bert = model  # allow bert to be config\n",
    "#         self.dropout = nn.Dropout(p=0.6)\n",
    "#         self.conv1=nn.Conv1D(768,1,20)\n",
    "#         self.classifier = nn.Linear(10,2)\n",
    "\n",
    "#     def forward(self,\n",
    "#                 input_ids,\n",
    "#                 token_type_ids=None,\n",
    "#                 position_ids=None,\n",
    "#                 attention_mask=None):\n",
    "#         outputs = self.bert(input_ids,\n",
    "#                             token_type_ids=token_type_ids,\n",
    "#                             position_ids=position_ids,\n",
    "#                             attention_mask=attention_mask)\n",
    "#         x= paddle.transpose(outputs, perm=[0, 2, 1])\n",
    "#         x=self.conv1(x)\n",
    "#         x=F.max_pool1d(x,kernel_size=4)\n",
    "#         x= paddle.transpose(x, perm=[0, 2, 1])\n",
    "#         x = self.dropout(x)\n",
    "#         x=x.reshape([x.shape[0],-1])\n",
    "#         logits = self.classifier(x)\n",
    "#         return logits\n",
    "# #model=Model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-06T11:49:30.812198Z",
     "iopub.status.idle": "2024-06-06T11:49:30.812484Z",
     "shell.execute_reply": "2024-06-06T11:49:30.812360Z",
     "shell.execute_reply.started": "2024-06-06T11:49:30.812349Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "\n",
    "class CrossEntropyCriterion(nn.Layer):\n",
    "    def __init__(self, label_smooth_eps=None,label_num=0):\n",
    "        \"\"\"\n",
    "        标签平滑的交叉熵损失\n",
    "        输入：\n",
    "            - label_smooth_eps: 标签平滑的参数\n",
    "            - label_num: 标签类别数\n",
    "        \"\"\"\n",
    "        super(CrossEntropyCriterion, self).__init__()\n",
    "        self.label_smooth_eps = label_smooth_eps\n",
    "        self.label_num= label_num\n",
    "\n",
    "    def forward(self, predict, label):\n",
    "        \"\"\"\n",
    "        前向计算\n",
    "        输入：\n",
    "            - predict: 模型的输出，维度为[批量大小, 标签类别数].\n",
    "            - label: 标签，维度为[批量大小, 标签类别数]\n",
    "        \"\"\"\n",
    "        # 标签平滑\n",
    "        #print(F.one_hot(x=label, num_classes=self.label_num))\n",
    "        label = F.label_smooth(label=F.one_hot(x=label.astype('int32'), num_classes=self.label_num),\\\n",
    "                                   epsilon=self.label_smooth_eps)\n",
    "        # 经过标签平滑后，label的维度为[批量大小, 序列长度, 目标语言词典大小]\n",
    "        # 交叉熵损失\n",
    "        loss = F.cross_entropy(\n",
    "            input=predict,\n",
    "            label=label,\n",
    "            reduction='none',\n",
    "            soft_label=True if self.label_smooth_eps else False)\n",
    "        # 返回 总损失，平均损失，非填充词元的数目\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-06T11:49:30.813896Z",
     "iopub.status.idle": "2024-06-06T11:49:30.814202Z",
     "shell.execute_reply": "2024-06-06T11:49:30.814059Z",
     "shell.execute_reply.started": "2024-06-06T11:49:30.814047Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from paddlenlp.data import Dict, Stack, Pad\n",
    "from paddle.io import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from visualdl import LogWriter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "\n",
    "stack= Stack()\n",
    "#定义模型训练器\n",
    "class Trainer():\n",
    "    def __init__(self,config):\n",
    "        self.config=config\n",
    "        self.input_maxlen=config['input_maxlen']  \n",
    "        self.label_maxlen=config['label_maxlen']\n",
    "        self.model_path=config['model_path']\n",
    "        self.model_savepath=config['model_savepath']\n",
    "        self.resume=config['resume']\n",
    "        self.log_path=config['log_path']  \n",
    "        self.best_score=0\n",
    "        # self.prompt=config['prompt']\n",
    "        self.model=config['model']\n",
    "        self.tokenizer=config['tokenizer']\n",
    "        self.optim=paddle.optimizer.AdamW(learning_rate=config['learning_rate'], \\\n",
    "                                        parameters=model.parameters(), \\\n",
    "                                        weight_decay=config['weight_decay'],\\\n",
    "                                         apply_decay_param_fun=lambda x: x in config['decay_params']) \n",
    "        #数据加载\n",
    "        self.train_dataloader=DataLoader(config['train_dataset'], batch_size=config['batch_size'],shuffle=True,drop_last=True,collate_fn=self.Collate_fn,num_workers=1)\n",
    "        self.val_dataloader=DataLoader(config['val_dataset'], batch_size=config['batch_size'],shuffle=True,drop_last=True,collate_fn=self.Collate_fn,num_workers=1)\n",
    "        self.loss_fct = CrossEntropyCriterion(0.4,2)\n",
    "        #定义\n",
    "        self.scheduler=paddle.optimizer.lr.LinearWarmup(learning_rate=config['learning_rate'], warmup_steps=len(self.train_dataloader), start_lr=0, end_lr=5e-5,verbose=True) \n",
    "    def Collate_fn(self,batch):     \n",
    "         #对输入字符串进行分词编码\n",
    "         input_ids_list=[]\n",
    "         attention_mask_list=[]\n",
    "         token_type_list=[]\n",
    "         pred_atten_list=[]\n",
    "         label_list=[]\n",
    "         for batch_dict in batch:\n",
    "              toponym=batch_dict['input']['toponym']\n",
    "              dfclass=batch_dict['input']['dfclass']\n",
    "              nfclass=batch_dict['input']['nfclass']\n",
    "              label=batch_dict['label']\n",
    "              #####test0######\n",
    "            #   inputs1='the place name is:{}.'.format(toponym)\n",
    "            #   inputs2=''\n",
    "              #########test1#####\n",
    "              #inputs1='the place name is:{}.'.format(toponym)\n",
    "              #inputs2='the category of derived place name:{}.'.format(dfclass)\n",
    "              ########test2#######\n",
    "              inputs1='the place name is:{}.'.format(toponym)\n",
    "              inputs2='the category of derived place name:{}.the category of native place name:{}.'.format(dfclass,nfclass)\n",
    "              input_dict=self.tokenizer(text=[inputs1],text_pair=[inputs2],pad=True,\\\n",
    "                                  add_special_tokens=True,\\\n",
    "                              max_seq_len=self.input_maxlen, \\\n",
    "                              pad_to_max_seq_len=True,truncation=True,return_attention_mask=True)\n",
    "    \n",
    "              pred_inputs=paddle.to_tensor(input_dict['input_ids'][0])\n",
    "              label_ids=paddle.to_tensor(label)\n",
    "              attention_mask=paddle.to_tensor(input_dict['attention_mask'][0])\n",
    "              token_type_id=paddle.to_tensor(input_dict['token_type_ids'][0])\n",
    "              input_ids_list.append(pred_inputs)\n",
    "              attention_mask_list.append(attention_mask)\n",
    "              token_type_list.append(token_type_id)\n",
    "              label_list.append(label_ids)\n",
    "         #拼接成一整张输入张量\n",
    "         input_ids_list=stack(input_ids_list)\n",
    "         attention_mask_list=stack(attention_mask_list)\n",
    "         label_list=stack(label_list)\n",
    "         return  {'input_ids':input_ids_list,'attention_mask':attention_mask_list,\\\n",
    "                                     'token_type_ids':token_type_list,'labels':label_list}\n",
    "\n",
    "    def load_model_optim_scheduler(self):\n",
    "        print('模型已加载！')\n",
    "        check_point_dict=paddle.load(os.path.join(self.model_savepath,'best_score.pkl'))\n",
    "        print(check_point_dict)\n",
    "        #self.best_score=check_point_dict['best_score']\n",
    "        self.start_epoch=check_point_dict['metric']['epoch']\n",
    "        self.model.set_state_dict(paddle.load(os.path.join(self.model_savepath,'model.pdparams')))\n",
    "        #self.optim.set_state_dict(paddle.load(os.path.join(self.model_savepath,'optim.pdopt')))\n",
    "        \n",
    "        return \n",
    "    def save_check_point(self,metric):\n",
    "        # paddle.save(self.model.state_dict(),os.path.join(self.model_savepath,'model.pdparams'))\n",
    "        self.model.train()\n",
    "        print('模型已保存！')\n",
    "        # 保存Layer参数\n",
    "        paddle.save(self.model.state_dict(), os.path.join(self.model_savepath,\"model.pdparams\"))\n",
    "        # 保存优化器参数\n",
    "        paddle.save(self.optim.state_dict(), os.path.join(self.model_savepath,'optim.pdopt'))\n",
    "        # 保存检查点checkpoint信息\n",
    "        paddle.save({'metric':metric},os.path.join(self.model_savepath,'best_score.pkl'))\n",
    "        return\n",
    "    def compute_loss(self,input_dict):\n",
    "        inputs={\n",
    "             'input_ids':input_dict['input_ids'],\n",
    "             'attention_mask':input_dict['attention_mask']\n",
    "                }\n",
    "        logits=self.model(**inputs)\n",
    "        labels=input_dict['labels'].reshape([-1])\n",
    "        loss=self.loss_fct(logits,labels)\n",
    "        return loss,logits\n",
    "        \n",
    "    def compute_metrics(self,input_dict):\n",
    "        __,logits=self.compute_loss(input_dict)\n",
    "        logits=logits.reshape([-1, logits.shape[-1]])\n",
    "        preds=paddle.argmax(logits,axis=-1)\n",
    "        label_ids=input_dict['labels'].reshape([-1])\n",
    "        #计算评价指标\n",
    "        accuracy = accuracy_score(label_ids.cpu(), preds.cpu())\n",
    "        precision = precision_score(label_ids.cpu(), preds.cpu(), average='weighted')\n",
    "        recall = recall_score(label_ids.cpu(), preds.cpu(), average='weighted')\n",
    "        f1 = f1_score(label_ids.cpu(), preds.cpu(), average='weighted') \n",
    "        return  accuracy,precision,recall,f1\n",
    "    def model_train_eval(self):\n",
    "        if self.resume:\n",
    "           self.load_model_optim_scheduler()\n",
    "        #初始化一个记录器\n",
    "        with LogWriter(logdir=self.log_path) as writer:\n",
    "            for epoch in range(self.config['epoch']):\n",
    "                #模型批次训练\n",
    "                loss_batch=self.model_batch_train()\n",
    "                # #更新学习率\n",
    "                self.scheduler.step()\n",
    "                print('epoch',epoch,\"train/loss\",loss_batch)\n",
    "                writer.add_scalar(tag=\"train/loss\", step=epoch, value=loss_batch)\n",
    "                # # #模型批次评估\n",
    "                acc,pre,rec,f1=self.model_batch_eval()\n",
    "                writer.add_scalar(tag=\"acc\", step=epoch, value=acc)\n",
    "                writer.add_scalar(tag=\"pre\", step=epoch, value=pre)\n",
    "                writer.add_scalar(tag=\"rec\", step=epoch, value=rec)\n",
    "                writer.add_scalar(tag=\"f1\", step=epoch, value=f1)\n",
    "                if  self.best_score<f1:\n",
    "                    self.best_score=f1\n",
    "                    metric={}\n",
    "                    metric['accuracy']= acc\n",
    "                    metric['precision']=pre\n",
    "                    metric['recall']=rec\n",
    "                    metric['f1']=f1\n",
    "                    metric['epoch']=epoch\n",
    "                    print('epoch',metric)\n",
    "                    self.save_check_point(metric)\n",
    "\n",
    "        return \n",
    "    def model_batch_train(self):\n",
    "        # 设置训练模式\n",
    "        self.model.train()\n",
    "        loss_batch=0\n",
    "        for batch_id,input_dict in enumerate(self.train_dataloader):\n",
    "            loss,logits=self.compute_loss(input_dict)\n",
    "            loss_batch+=loss.item()\n",
    "            #梯度清零\n",
    "            self.optim.clear_grad()\n",
    "            #计算梯度，反向传播\n",
    "            loss.backward()\n",
    "            #更新参数\n",
    "            self.optim.step()\n",
    "        loss_batch/=(batch_id+1)\n",
    "        return  loss_batch\n",
    "    @paddle.no_grad()\n",
    "    def model_batch_eval(self):\n",
    "        #设置评估模式\n",
    "        self.model.eval()\n",
    "        all_preds=[]\n",
    "        all_labels=[]\n",
    "        acc_eval,pre_eval,rec_eval,f1_eval=0,0,0,0\n",
    "        for batch_id, input_dict in enumerate(self.val_dataloader):\n",
    "             accuracy,precision,recall,f1=self.compute_metrics(input_dict)\n",
    "             acc_eval+=accuracy\n",
    "             pre_eval+=precision\n",
    "             rec_eval+=recall\n",
    "             f1_eval+=f1\n",
    "        return  acc_eval/(batch_id+1),pre_eval/(batch_id+1),rec_eval/(batch_id+1),f1_eval/(batch_id+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-06T11:49:30.815493Z",
     "iopub.status.idle": "2024-06-06T11:49:30.815815Z",
     "shell.execute_reply": "2024-06-06T11:49:30.815682Z",
     "shell.execute_reply.started": "2024-06-06T11:49:30.815669Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#模型训练配置\n",
    "config={\n",
    "        'model_savepath':r\"/home/aistudio/work/ernie_prompt2\",\n",
    "        'log_path':'./log/ernie_prompt2',\n",
    "        # 指定优化策略，更新模型参数\n",
    "        'decay_params':[p.name for n, p in model.named_parameters() if not any(nd in n for nd in [\"bias\", \"norm\"])],\n",
    "        'model_path':r'/home/aistudio/work/output',\n",
    "        #'prompt':'the place name is:{}.',#test0\n",
    "        #'prompt':'the category of place name:{}.the place name is:{}.',#test2\n",
    "        'input_maxlen':60,\n",
    "        'label_maxlen':10,\n",
    "        'resume':False,\n",
    "        'model':model,\n",
    "        'tokenizer':tokenizer,\n",
    "         'epoch':50,\n",
    "         'batch_size':128,\n",
    "         'learning_rate':5e-8,\n",
    "         'weight_decay':0.1,\n",
    "         'train_dataset':trainDataset,\n",
    "         'val_dataset':valDataset}\n",
    "#训练器初始化\n",
    "model_trainer=Trainer(config)\n",
    "model_trainer.model_train_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-06T11:49:30.816932Z",
     "iopub.status.idle": "2024-06-06T11:49:30.817221Z",
     "shell.execute_reply": "2024-06-06T11:49:30.817099Z",
     "shell.execute_reply.started": "2024-06-06T11:49:30.817087Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "model_savepath=r\"/home/aistudio/work/ernie_prompt2\"\n",
    "check_point_dict=paddle.load(os.path.join(model_savepath,'best_score.pkl'))\n",
    "print(check_point_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
